19/03/08 02:22:23 WARN MemoryStore: Not enough space to cache rdd_1427_0 in memory! (computed 150.5 MB so far)
19/03/08 02:22:23 WARN BlockManager: Persisting block rdd_1427_0 to disk instead.
19/03/08 02:22:26 WARN MemoryStore: Not enough space to cache rdd_1427_1 in memory! (computed 150.5 MB so far)
19/03/08 02:22:26 WARN BlockManager: Persisting block rdd_1427_1 to disk instead.
19/03/08 02:22:34 WARN MemoryStore: Not enough space to cache rdd_1427_1 in memory! (computed 231.1 MB so far)
19/03/08 02:22:47 WARN MemoryStore: Not enough space to cache rdd_1427_0 in memory! (computed 97.6 MB so far)
19/03/08 02:23:09 WARN BlockManager: Block rdd_1427_2 could not be removed as it was not found on disk or in memory
19/03/08 02:23:09 ERROR Executor: Exception in task 2.0 in stage 371.0 (TID 5625)
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.lang.Double.valueOf(Double.java:519)
	at scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:79)
	at breeze.storage.Zero$DoubleZero$.zero(Zero.scala:80)
	at breeze.linalg.SparseVector$mcD$sp.<init>(SparseVector.scala:55)
	at breeze.linalg.SparseVector$mcD$sp.<init>(SparseVector.scala:57)
	at org.apache.spark.ml.linalg.SparseVector.asBreeze(Vectors.scala:604)
	at org.apache.spark.ml.linalg.Vector$class.apply(Vectors.scala:102)
	at org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:561)
	at org.apache.spark.ml.tree.impl.TreePoint$.findBin(TreePoint.scala:112)
	at org.apache.spark.ml.tree.impl.TreePoint$.org$apache$spark$ml$tree$impl$TreePoint$$labeledPointToTreePoint(TreePoint.scala:93)
	at org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:73)
	at org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:72)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
19/03/08 02:23:09 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 5625,5,main]
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.lang.Double.valueOf(Double.java:519)
	at scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:79)
	at breeze.storage.Zero$DoubleZero$.zero(Zero.scala:80)
	at breeze.linalg.SparseVector$mcD$sp.<init>(SparseVector.scala:55)
	at breeze.linalg.SparseVector$mcD$sp.<init>(SparseVector.scala:57)
	at org.apache.spark.ml.linalg.SparseVector.asBreeze(Vectors.scala:604)
	at org.apache.spark.ml.linalg.Vector$class.apply(Vectors.scala:102)
	at org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:561)
	at org.apache.spark.ml.tree.impl.TreePoint$.findBin(TreePoint.scala:112)
	at org.apache.spark.ml.tree.impl.TreePoint$.org$apache$spark$ml$tree$impl$TreePoint$$labeledPointToTreePoint(TreePoint.scala:93)
	at org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:73)
	at org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:72)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
19/03/08 02:23:09 WARN TaskSetManager: Lost task 2.0 in stage 371.0 (TID 5625, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.lang.Double.valueOf(Double.java:519)
	at scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:79)
	at breeze.storage.Zero$DoubleZero$.zero(Zero.scala:80)
	at breeze.linalg.SparseVector$mcD$sp.<init>(SparseVector.scala:55)
	at breeze.linalg.SparseVector$mcD$sp.<init>(SparseVector.scala:57)
	at org.apache.spark.ml.linalg.SparseVector.asBreeze(Vectors.scala:604)
	at org.apache.spark.ml.linalg.Vector$class.apply(Vectors.scala:102)
	at org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:561)
	at org.apache.spark.ml.tree.impl.TreePoint$.findBin(TreePoint.scala:112)
	at org.apache.spark.ml.tree.impl.TreePoint$.org$apache$spark$ml$tree$impl$TreePoint$$labeledPointToTreePoint(TreePoint.scala:93)
	at org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:73)
	at org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:72)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

19/03/08 02:23:09 ERROR TaskSetManager: Task 2 in stage 371.0 failed 1 times; aborting job
19/03/08 02:24:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/03/08 02:26:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/03/08 02:28:28 WARN MemoryStore: Not enough space to cache rdd_248_0 in memory! (computed 347.6 MB so far)
19/03/08 02:28:29 WARN MemoryStore: Not enough space to cache rdd_248_1 in memory! (computed 347.6 MB so far)
19/03/08 02:28:29 WARN BlockManager: Persisting block rdd_248_0 to disk instead.
19/03/08 02:28:29 WARN BlockManager: Persisting block rdd_248_1 to disk instead.
19/03/08 02:29:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/03/08 02:35:23 WARN MemoryStore: Not enough space to cache rdd_369_0 in memory! (computed 150.5 MB so far)
19/03/08 02:35:23 WARN BlockManager: Persisting block rdd_369_0 to disk instead.
19/03/08 02:35:24 WARN MemoryStore: Not enough space to cache rdd_369_1 in memory! (computed 150.5 MB so far)
19/03/08 02:35:24 WARN BlockManager: Persisting block rdd_369_1 to disk instead.
19/03/08 02:35:32 WARN MemoryStore: Not enough space to cache rdd_369_1 in memory! (computed 231.1 MB so far)
19/03/08 02:35:40 WARN MemoryStore: Not enough space to cache rdd_369_2 in memory! (computed 150.5 MB so far)
19/03/08 02:35:40 WARN BlockManager: Persisting block rdd_369_2 to disk instead.
19/03/08 02:35:40 WARN MemoryStore: Not enough space to cache rdd_369_0 in memory! (computed 150.5 MB so far)
19/03/08 02:35:49 WARN MemoryStore: Not enough space to cache rdd_369_3 in memory! (computed 231.1 MB so far)
19/03/08 02:35:49 WARN BlockManager: Persisting block rdd_369_3 to disk instead.
19/03/08 02:36:16 WARN MemoryStore: Not enough space to cache rdd_369_3 in memory! (computed 347.6 MB so far)
19/03/08 02:36:33 WARN MemoryStore: Not enough space to cache rdd_369_6 in memory! (computed 231.1 MB so far)
19/03/08 02:36:33 WARN BlockManager: Persisting block rdd_369_6 to disk instead.
19/03/08 02:36:38 WARN MemoryStore: Not enough space to cache rdd_369_2 in memory! (computed 347.6 MB so far)
19/03/08 02:36:57 WARN MemoryStore: Not enough space to cache rdd_369_6 in memory! (computed 347.6 MB so far)
19/03/08 02:36:58 WARN MemoryStore: Not enough space to cache rdd_369_7 in memory! (computed 9.4 MB so far)
19/03/08 02:36:58 WARN BlockManager: Persisting block rdd_369_7 to disk instead.
19/03/08 02:37:13 WARN MemoryStore: Not enough space to cache rdd_369_7 in memory! (computed 347.6 MB so far)
19/03/08 02:37:18 WARN MemoryStore: Not enough space to cache rdd_369_1 in memory! (computed 150.5 MB so far)
19/03/08 02:37:18 WARN MemoryStore: Not enough space to cache rdd_369_0 in memory! (computed 150.5 MB so far)
19/03/08 02:37:22 WARN MemoryStore: Not enough space to cache rdd_369_2 in memory! (computed 231.1 MB so far)
19/03/08 02:37:24 WARN MemoryStore: Not enough space to cache rdd_369_3 in memory! (computed 347.6 MB so far)
19/03/08 02:37:33 WARN MemoryStore: Not enough space to cache rdd_369_7 in memory! (computed 18.2 MB so far)
19/03/08 02:37:33 WARN MemoryStore: Not enough space to cache rdd_369_6 in memory! (computed 347.6 MB so far)
19/03/08 02:37:41 WARN MemoryStore: Not enough space to cache rdd_369_0 in memory! (computed 150.5 MB so far)
19/03/08 02:37:42 WARN MemoryStore: Not enough space to cache rdd_369_1 in memory! (computed 150.5 MB so far)
19/03/08 02:37:45 WARN MemoryStore: Not enough space to cache rdd_369_2 in memory! (computed 347.6 MB so far)
19/03/08 02:37:47 WARN MemoryStore: Not enough space to cache rdd_369_3 in memory! (computed 18.2 MB so far)
19/03/08 02:37:55 WARN MemoryStore: Not enough space to cache rdd_369_6 in memory! (computed 347.6 MB so far)
19/03/08 02:37:56 WARN MemoryStore: Not enough space to cache rdd_369_7 in memory! (computed 18.2 MB so far)
19/03/08 02:38:03 WARN MemoryStore: Not enough space to cache rdd_369_1 in memory! (computed 150.5 MB so far)
19/03/08 02:38:03 WARN MemoryStore: Not enough space to cache rdd_369_0 in memory! (computed 150.5 MB so far)
19/03/08 02:38:07 WARN MemoryStore: Not enough space to cache rdd_369_2 in memory! (computed 347.6 MB so far)
19/03/08 02:38:10 WARN MemoryStore: Not enough space to cache rdd_369_3 in memory! (computed 347.6 MB so far)
19/03/08 02:38:20 WARN MemoryStore: Not enough space to cache rdd_369_6 in memory! (computed 231.1 MB so far)
19/03/08 02:38:20 WARN MemoryStore: Not enough space to cache rdd_369_7 in memory! (computed 97.6 MB so far)
19/03/08 02:38:29 WARN MemoryStore: Not enough space to cache rdd_369_1 in memory! (computed 150.5 MB so far)
19/03/08 02:38:30 WARN MemoryStore: Not enough space to cache rdd_369_0 in memory! (computed 150.5 MB so far)
19/03/08 02:38:33 WARN MemoryStore: Not enough space to cache rdd_369_2 in memory! (computed 231.1 MB so far)
19/03/08 02:38:35 WARN MemoryStore: Not enough space to cache rdd_369_3 in memory! (computed 347.6 MB so far)
19/03/08 02:38:40 ERROR Executor: Exception in task 2.0 in stage 111.0 (TID 266)
java.lang.OutOfMemoryError: Java heap space
	at java.lang.reflect.Array.newInstance(Array.java:75)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1939)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.storage.memory.PartiallyUnrolledIterator.hasNext(MemoryStore.scala:732)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.ml.tree.impl.RandomForest$$anonfun$12.apply(RandomForest.scala:545)
	at org.apache.spark.ml.tree.impl.RandomForest$$anonfun$12.apply(RandomForest.scala:534)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
19/03/08 02:38:40 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 266,5,main]
java.lang.OutOfMemoryError: Java heap space
	at java.lang.reflect.Array.newInstance(Array.java:75)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1939)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.storage.memory.PartiallyUnrolledIterator.hasNext(MemoryStore.scala:732)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.ml.tree.impl.RandomForest$$anonfun$12.apply(RandomForest.scala:545)
	at org.apache.spark.ml.tree.impl.RandomForest$$anonfun$12.apply(RandomForest.scala:534)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
19/03/08 02:38:40 WARN TaskSetManager: Lost task 2.0 in stage 111.0 (TID 266, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space
	at java.lang.reflect.Array.newInstance(Array.java:75)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1939)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.storage.memory.PartiallyUnrolledIterator.hasNext(MemoryStore.scala:732)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.ml.tree.impl.RandomForest$$anonfun$12.apply(RandomForest.scala:545)
	at org.apache.spark.ml.tree.impl.RandomForest$$anonfun$12.apply(RandomForest.scala:534)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)

19/03/08 02:38:40 ERROR TaskSetManager: Task 2 in stage 111.0 failed 1 times; aborting job
19/03/08 02:38:40 WARN TaskSetManager: Lost task 3.0 in stage 111.0 (TID 267, localhost, executor driver): TaskKilled (Stage cancelled)
19/03/08 02:42:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/03/08 02:44:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/03/08 02:46:49 WARN MemoryStore: Not enough space to cache rdd_224_0 in memory! (computed 347.6 MB so far)
19/03/08 02:46:49 WARN MemoryStore: Not enough space to cache rdd_224_1 in memory! (computed 347.6 MB so far)
19/03/08 02:46:49 WARN BlockManager: Persisting block rdd_224_0 to disk instead.
19/03/08 02:46:49 WARN BlockManager: Persisting block rdd_224_1 to disk instead.
19/03/08 02:48:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/03/08 02:50:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/03/08 02:52:02 WARN MemoryStore: Not enough space to cache rdd_224_0 in memory! (computed 150.5 MB so far)
19/03/08 02:52:03 WARN BlockManager: Persisting block rdd_224_0 to disk instead.
19/03/08 02:52:04 WARN MemoryStore: Not enough space to cache rdd_224_1 in memory! (computed 150.5 MB so far)
19/03/08 02:52:04 WARN BlockManager: Persisting block rdd_224_1 to disk instead.
19/03/08 02:52:13 WARN MemoryStore: Not enough space to cache rdd_224_1 in memory! (computed 231.1 MB so far)
19/03/08 02:52:24 WARN MemoryStore: Not enough space to cache rdd_224_2 in memory! (computed 231.1 MB so far)
19/03/08 02:52:24 WARN BlockManager: Persisting block rdd_224_2 to disk instead.
19/03/08 02:52:32 WARN MemoryStore: Not enough space to cache rdd_224_0 in memory! (computed 347.6 MB so far)
19/03/08 02:52:43 WARN BlockManager: Block rdd_224_2 could not be removed as it was not found on disk or in memory
19/03/08 02:52:43 ERROR Executor: Exception in task 2.0 in stage 58.0 (TID 121)
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at breeze.linalg.SparseVector$mcD$sp.<init>(SparseVector.scala:55)
	at breeze.linalg.SparseVector$mcD$sp.<init>(SparseVector.scala:57)
	at org.apache.spark.ml.linalg.SparseVector.asBreeze(Vectors.scala:604)
	at org.apache.spark.ml.linalg.Vector$class.apply(Vectors.scala:102)
	at org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:561)
	at org.apache.spark.ml.tree.impl.TreePoint$.findBin(TreePoint.scala:112)
	at org.apache.spark.ml.tree.impl.TreePoint$.org$apache$spark$ml$tree$impl$TreePoint$$labeledPointToTreePoint(TreePoint.scala:93)
	at org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:73)
	at org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:72)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:743)
	at org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)
	at org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:174)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1103)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1101)
	at org.apache.spark.storage.DiskStore.put(DiskStore.scala:69)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1101)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
19/03/08 02:52:43 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 121,5,main]
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at breeze.linalg.SparseVector$mcD$sp.<init>(SparseVector.scala:55)
	at breeze.linalg.SparseVector$mcD$sp.<init>(SparseVector.scala:57)
	at org.apache.spark.ml.linalg.SparseVector.asBreeze(Vectors.scala:604)
	at org.apache.spark.ml.linalg.Vector$class.apply(Vectors.scala:102)
	at org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:561)
	at org.apache.spark.ml.tree.impl.TreePoint$.findBin(TreePoint.scala:112)
	at org.apache.spark.ml.tree.impl.TreePoint$.org$apache$spark$ml$tree$impl$TreePoint$$labeledPointToTreePoint(TreePoint.scala:93)
	at org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:73)
	at org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:72)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:743)
	at org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)
	at org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:174)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1103)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1101)
	at org.apache.spark.storage.DiskStore.put(DiskStore.scala:69)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1101)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
19/03/08 02:52:43 WARN TaskSetManager: Lost task 2.0 in stage 58.0 (TID 121, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at breeze.linalg.SparseVector$mcD$sp.<init>(SparseVector.scala:55)
	at breeze.linalg.SparseVector$mcD$sp.<init>(SparseVector.scala:57)
	at org.apache.spark.ml.linalg.SparseVector.asBreeze(Vectors.scala:604)
	at org.apache.spark.ml.linalg.Vector$class.apply(Vectors.scala:102)
	at org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:561)
	at org.apache.spark.ml.tree.impl.TreePoint$.findBin(TreePoint.scala:112)
	at org.apache.spark.ml.tree.impl.TreePoint$.org$apache$spark$ml$tree$impl$TreePoint$$labeledPointToTreePoint(TreePoint.scala:93)
	at org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:73)
	at org.apache.spark.ml.tree.impl.TreePoint$$anonfun$convertToTreeRDD$2.apply(TreePoint.scala:72)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:743)
	at org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)
	at org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:174)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1103)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1101)
	at org.apache.spark.storage.DiskStore.put(DiskStore.scala:69)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1101)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)

19/03/08 02:52:43 ERROR TaskSetManager: Task 2 in stage 58.0 failed 1 times; aborting job
