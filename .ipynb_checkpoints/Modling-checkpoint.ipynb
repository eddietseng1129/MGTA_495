{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os.path\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpathA = 'ecommerce/order_items_dataset.csv'\n",
    "fpathB = 'ecommerce/customers_dataset.csv'\n",
    "fpathC = 'ecommerce/geolocation_dataset.csv'\n",
    "fpathD = 'ecommerce/order_payments_dataset.csv'\n",
    "fpathE = 'ecommerce/orders_dataset.csv'\n",
    "fpathF = 'ecommerce/products_dataset.csv'\n",
    "fpathG = 'ecommerce/sellers_dataset.csv'\n",
    "fpathH = 'ecommerce/product_category_name_translation.csv'\n",
    "fpathI = 'ecommerce/customer_reviews_dataset.csv'\n",
    "\n",
    "orders = spark.read.csv(fpathE, inferSchema=\"true\", header=\"true\")\n",
    "orderItems = spark.read.csv(fpathA, inferSchema=\"true\", header=\"true\")\n",
    "customerReviews = spark.read.csv(fpathI, inferSchema=\"true\", header=\"true\")\n",
    "products = spark.read.csv(fpathF, inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('orderItems rows: ', orderItems.count())\n",
    "print('order rows: ', orders.count())\n",
    "print('customerReviews rows: ', customerReviews.count())\n",
    "print('products rows: ', products.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the difference between deliver time and estimate time \n",
    "timeFmt = \"yyyy-MM-dd HH:mm:ss\"\n",
    "timeDiff = (f.unix_timestamp('order_estimated_delivery_date', format=timeFmt)\n",
    "            - f.unix_timestamp('order_customer_delivery_date', format=timeFmt))\n",
    "diff = orders['order_id', 'order_customer_delivery_date', 'order_estimated_delivery_date'].dropna(how='any')\n",
    "diff = diff.withColumn(\"Duration\", timeDiff)\n",
    "diff.show(3)\n",
    "mergedOrders = orders.join(diff, on=['order_id'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames_order = ['order_id', 'order_status', 'Duration']\n",
    "colNames_items = ['order_id', 'product_id', 'price', 'freight_value'] # , 'order_item_id'\n",
    "colNames_reviews = ['order_id', 'survey_score'] # 'review_id'\n",
    "colNames_product = ['product_id', 'product_category_name']\n",
    "mergedOrdersDF = mergedOrders[colNames_order]\n",
    "orderItemsDF = orderItems[colNames_items]\n",
    "customerReviewsDF = customerReviews[colNames_reviews]\n",
    "productDF = products[colNames_product]\n",
    "print('# of rows in mergedOrdersDF: ', mergedOrdersDF.count())\n",
    "print('# of rows in orderItemsDF: ', orderItemsDF.count())\n",
    "print('# of rows in customerReviewsDF: ', customerReviewsDF.count())\n",
    "print('# of rows in productDF: ', productDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge product and items with product_id\n",
    "tempDF = orderItemsDF.join(productDF, on=['product_id'], how='inner')\n",
    "# merge dataframe by aligning order_id\n",
    "tempDF2 =  mergedOrdersDF.join(tempDF, on=['order_id'], how='inner')\n",
    "mergedDF = customerReviewsDF.join(tempDF2, on=['order_id'], how='inner')\n",
    "# mergedDF.show(5)\n",
    "print('# of rows in mergedDF: ', mergedDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean mergedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove null\n",
    "cleanDF = mergedDF.dropna(how='any')\n",
    "print('# of rows remaining: ', cleanDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "cleanDF = cleanDF.withColumn(\"survey_score\", cleanDF[\"survey_score\"].cast(DoubleType()))\n",
    "print('Unbalanced data')\n",
    "# cleanDF.select('survey_score').toPandas().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using order_id, product_id, review_id, price, freight_value to predict the survey_score\n",
    "featureCols = ['freight_value', 'Duration', 'price'] # 'order_id', 'order_item_id', 'product_id', , 'product_category_name'\n",
    "labelCol = 'survey_score'\n",
    "feature_data = cleanDF[featureCols + [labelCol]].dropna(how='any')\n",
    "print('Number of features: ', len(featureCols))\n",
    "print('Number of rows in cleaned DataFrame: ', feature_data.count())\n",
    "# changedTypedf = feature_data.withColumn(\"survey_score\", feature_data[\"survey_score\"].cast(DoubleType()))\n",
    "# changedTypedf.printSchema()\n",
    "# feature_data = changedTypedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehot encoding for id\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "cols = featureCols\n",
    "# cols = ['product_id', 'freight_value', 'Duration', 'price', 'product_category_name'] # 'order_id', 'order_item_id', 'freight_value'\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "    for c in cols\n",
    "]\n",
    "\n",
    "encoders = [\n",
    "    OneHotEncoder(\n",
    "        inputCol=indexer.getOutputCol(),\n",
    "        outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "vector_data = pipeline.fit(feature_data).transform(feature_data)['features', labelCol]# .show(3)\n",
    "vector_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF, testDf = vector_data.randomSplit([0.85, 0.15], seed=12345)\n",
    "print('trainDF')\n",
    "trainDF.show(5)\n",
    "print('testDf')\n",
    "testDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from numpy.random import randint\n",
    "from pyspark.sql.functions import isnan, when, count, col, rand, isnull, avg, stddev, udf, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dowm_resample(base_features,ratio,class_field,base_class):\n",
    "    pos = base_features.filter(col(class_field)==base_class)\n",
    "    neg1 = base_features.filter(col(class_field)==1.0)\n",
    "    neg2 = base_features.filter(col(class_field)==5.0)\n",
    "    neg3 = base_features.filter(col(class_field)==3.0)\n",
    "    neg4 = base_features.filter(col(class_field)==4.0)\n",
    "    total_pos = pos.count()\n",
    "    total_neg1 = neg1.count()\n",
    "    total_neg2 = neg2.count()\n",
    "    total_neg3 = neg3.count()\n",
    "    total_neg4 = neg4.count()\n",
    "    fraction1=float(total_pos*ratio)/float(total_neg1)\n",
    "    fraction2=float(total_pos*ratio)/float(total_neg2)\n",
    "    fraction3=float(total_pos*ratio)/float(total_neg3)\n",
    "    fraction4=float(total_pos*ratio)/float(total_neg4)\n",
    "    print(fraction1, fraction2, fraction3, fraction4)\n",
    "    sampled1 = neg1.sample(False,fraction1)\n",
    "    sampled2 = neg2.sample(False,fraction2)\n",
    "    sampled3 = neg3.sample(False,fraction3)\n",
    "    sampled4 = neg4.sample(False,fraction4)\n",
    "    sampled1 = sampled1.union(pos)\n",
    "    sampled2 = sampled2.union(sampled1)\n",
    "    sampled3 = sampled3.union(sampled2)\n",
    "    return sampled4.union(sampled3)\n",
    "\n",
    "ratio = 1\n",
    "resampleDF = dowm_resample(trainDF, ratio, 'survey_score', 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression (Predict score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor, GBTRegressor, GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBTRegressor (Unbalanced Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = GBTRegressor(featuresCol ='features', labelCol = labelCol)\n",
    "dt_model = dt.fit(trainDF)\n",
    "dt_predictions = dt_model.transform(testDf)\n",
    "dt_evaluator = RegressionEvaluator(labelCol=labelCol, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_predictions.select('prediction').toPandas().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBTRegressor (Balanced Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = GBTRegressor(featuresCol ='features', labelCol = labelCol)\n",
    "dt_model = dt.fit(resampleDF)\n",
    "dt_predictions = dt_model.transform(testDf)\n",
    "dt_evaluator = RegressionEvaluator(labelCol=labelCol, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions) \n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_predictions.select(\"prediction\",labelCol,\"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeRegressor (Unbalanced Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = labelCol)\n",
    "dt_model = dt.fit(trainDF)\n",
    "dt_predictions = dt_model.transform(testDf)\n",
    "dt_evaluator = RegressionEvaluator(labelCol=labelCol, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_predictions.select(\"prediction\",labelCol,\"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeRegressor (Balanced Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = labelCol)\n",
    "dt_model = dt.fit(resampleDF)\n",
    "dt_predictions = dt_model.transform(testDf)\n",
    "dt_evaluator = RegressionEvaluator(labelCol=labelCol, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_predictions.select('prediction').toPandas().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP and CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpathA = 'ecommerce/order_items_dataset.csv'\n",
    "fpathB = 'ecommerce/customers_dataset.csv'\n",
    "fpathC = 'ecommerce/geolocation_dataset.csv'\n",
    "fpathD = 'ecommerce/order_payments_dataset.csv'\n",
    "fpathE = 'ecommerce/orders_dataset.csv'\n",
    "fpathF = 'ecommerce/products_dataset.csv'\n",
    "fpathG = 'ecommerce/sellers_dataset.csv'\n",
    "fpathH = 'ecommerce/product_category_name_translation.csv'\n",
    "fpathI = 'ecommerce/customer_reviews_dataset.csv'\n",
    "\n",
    "orders = pd.read_csv(fpathE)\n",
    "orderItems = pd.read_csv(fpathA)\n",
    "customerReviews = pd.read_csv(fpathI)\n",
    "products = pd.read_csv(fpathF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import  datetime\n",
    "orders['order_estimated_delivery_date'] = pd.to_datetime(orders['order_estimated_delivery_date'])\n",
    "orders['order_customer_delivery_date'] = pd.to_datetime(orders['order_customer_delivery_date'])\n",
    "duration = (orders.order_customer_delivery_date - orders.order_estimated_delivery_date).astype(int)\n",
    "duration = pd.Series.to_frame(duration)\n",
    "orders['duration'] = duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedOrdersDF = orders[['order_id', 'order_status', 'duration']]\n",
    "orderItemsDF = orderItems[['order_id', 'product_id', 'price', 'freight_value']]\n",
    "customerReviewsDF = customerReviews[['order_id', 'survey_score']]\n",
    "productDF = products[['product_id', 'product_category_name']]\n",
    "tempDF = orderItemsDF.merge(productDF, on=['product_id'], how='outer')\n",
    "tempDF2 =  mergedOrdersDF.merge(tempDF, on=['order_id'], how='outer')\n",
    "mergedDF = customerReviewsDF.merge(tempDF2, on=['order_id'], how='outer')\n",
    "mergedDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDF = mergedDF.dropna(how='any')\n",
    "mergedDF = mergedDF[['survey_score', 'order_status', 'duration', 'product_id', 'price', 'freight_value', 'product_category_name']]\n",
    "mergedDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras_pandas.Automater import Automater\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train /test split\n",
    "train_observations, test_observations = train_test_split(mergedDF, test_size=0.001, random_state=42)\n",
    "train_observations = train_observations.copy()\n",
    "test_observations = test_observations.copy()\n",
    "train_observations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data set, using keras_pandas\n",
    "categorical_vars = ['order_status', 'product_category_name']\n",
    "numerical_vars = ['duration', 'price', 'freight_value', 'survey_score']\n",
    "text_vars = ['product_id']\n",
    "\n",
    "data_type_dict = {'numerical': numerical_vars, 'categorical': categorical_vars, 'text': text_vars}\n",
    "output_var = 'survey_score'\n",
    "auto = Automater(data_type_dict=data_type_dict, output_var=output_var)\n",
    "auto.fit(train_observations)\n",
    "\n",
    "# Transform data\n",
    "train_X, train_y = auto.fit_transform(train_observations)\n",
    "test_X, test_y = auto.transform(test_observations)\n",
    "\n",
    "# Create and fit keras (deep learning) model.\n",
    "x = auto.input_nub\n",
    "\n",
    "# Fill in your own hidden layers\n",
    "# x = Dense(14, kernel_initializer='normal')(x)\n",
    "# x = Dropout(rate= 0.1)(x)\n",
    "x = Dense(12, activation = 'relu', kernel_initializer = 'uniform')(x)\n",
    "x = Dropout(rate= 0.2)(x)\n",
    "x = Dense(6, activation='relu', kernel_initializer = 'uniform')(x)\n",
    "x = Dropout(rate= 0.2)(x)\n",
    "x = Dense(1, activation='linear', kernel_initializer = 'uniform')(x)\n",
    "\n",
    "x = auto.output_nub(x)\n",
    "\n",
    "model = Model(inputs=auto.input_layers, outputs=x)\n",
    "model.compile(loss=auto.suggest_loss(), optimizer='adam', metrics=['mse','mae'])\n",
    "history = model.fit(train_X, train_y, epochs=10, batch_size=150, validation_split=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('CNN model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Approach (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import udf, col, avg, mean, stddev\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as f\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "# import data_dict_loader\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = '../result2.csv'\n",
    "result2_df = spark.read.load(result2,format=\"csv\",inferSchema=\"true\",header=\"false\").cache()\n",
    "result2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df2 = result2_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol=\"_c8\", outputCol=\"categoryIndex\")\n",
    "model = stringIndexer.fit(cleaned_df2)\n",
    "indexed = model.transform(cleaned_df2)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"_c1\", outputCol=\"stateIndex\")\n",
    "model = stringIndexer.fit(encoded)\n",
    "indexed = model.transform(encoded)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"stateIndex\", outputCol=\"stateVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "min_sc = encoded.select(f.min(encoded._c7)).collect()[0][0]\n",
    "max_sc = encoded.select(f.max(encoded._c7)).collect()[0][0]\n",
    "splits = list(range(int(min_sc), int(max_sc) + 1, 1))\n",
    "bucketizer = Bucketizer(splits=splits, inputCol= '_c7', outputCol=\"Bucketed\")\n",
    "bucket_df = bucketizer.transform(encoded)\n",
    "bucket_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureCols = ['stateIndex', '_c2','_c5','_c6','_c4', '_c3','_c9']\n",
    "labelCol = 'categoryIndex'\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=featureCols,\n",
    "    outputCol=\"features\")\n",
    "assembled_df = assembler.transform(encoded)[['features', 'categoryIndex']]\n",
    "assembled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = assembled_df.randomSplit([0.75, 0.25], 24)\n",
    "train_df = tmp[0]\n",
    "test_df = tmp[1]\n",
    "print('{} = {} + {}'.format(\n",
    "    train_df.count() + test_df.count(),\n",
    "    train_df.count(),\n",
    "    test_df.count()))\n",
    "train_df.show(5)\n",
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(labelCol='categoryIndex', featuresCol=\"features\", maxDepth=15)\n",
    "model_no_tune = model.fit(train_df)\n",
    "print(model_no_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "train_pred = model_no_tune.transform(train_df)\n",
    "test_pred = model_no_tune.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"categoryIndex\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\")\n",
    "train_accu = evaluator.evaluate(train_pred)\n",
    "test_accu = evaluator.evaluate(test_pred)\n",
    "print('Train accuracy:', train_accu)\n",
    "print('Test accuracy:', test_accu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
